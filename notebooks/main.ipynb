{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stanleysalvatierra/anaconda3/envs/vision_p310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from PIL import Image\n",
    "import glob\n",
    "import torch\n",
    "import pickle\n",
    "import zipfile\n",
    "from IPython.display import display\n",
    "from IPython.display import Image as IPImage\n",
    "import os\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have implemented our own, efficient method\n",
    "# to find high density regions in vector space\n",
    "def community_detection(embeddings, threshold, min_community_size=10, init_max_size=100):\n",
    "    \"\"\"\n",
    "    Function for Fast Community Detection\n",
    "\n",
    "    Finds in the embeddings all communities, i.e. embeddings that are close (closer than threshold).\n",
    "\n",
    "    Returns only communities that are larger than min_community_size. The communities are returned\n",
    "    in decreasing order. The first element in each list is the central point in the community.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute cosine similarity scores\n",
    "    cos_scores = util.cos_sim(embeddings, embeddings)\n",
    "\n",
    "    # Minimum size for a community\n",
    "    top_k_values, _ = cos_scores.topk(k=min_community_size, largest=True)\n",
    "\n",
    "    # Filter for rows >= min_threshold\n",
    "    extracted_communities = []\n",
    "    for i in range(len(top_k_values)):\n",
    "        if top_k_values[i][-1] >= threshold:\n",
    "            new_cluster = []\n",
    "\n",
    "            # Only check top k most similar entries\n",
    "            top_val_large, top_idx_large = cos_scores[i].topk(k=init_max_size, largest=True)\n",
    "            top_idx_large = top_idx_large.tolist()\n",
    "            top_val_large = top_val_large.tolist()\n",
    "\n",
    "            if top_val_large[-1] < threshold:\n",
    "                for idx, val in zip(top_idx_large, top_val_large):\n",
    "                    if val < threshold:\n",
    "                        break\n",
    "\n",
    "                    new_cluster.append(idx)\n",
    "            else:\n",
    "                # Iterate over all entries (slow)\n",
    "                for idx, val in enumerate(cos_scores[i].tolist()):\n",
    "                    if val >= threshold:\n",
    "                        new_cluster.append(idx)\n",
    "\n",
    "            extracted_communities.append(new_cluster)\n",
    "\n",
    "    # Largest cluster first\n",
    "    extracted_communities = sorted(extracted_communities, key=lambda x: len(x), reverse=True)\n",
    "\n",
    "    # Step 2) Remove overlapping communities\n",
    "    unique_communities = []\n",
    "    extracted_ids = set()\n",
    "\n",
    "    for community in extracted_communities:\n",
    "        add_cluster = True\n",
    "        for idx in community:\n",
    "            if idx in extracted_ids:\n",
    "                add_cluster = False\n",
    "                break\n",
    "\n",
    "        if add_cluster:\n",
    "            unique_communities.append(community)\n",
    "            for idx in community:\n",
    "                extracted_ids.add(idx)\n",
    "\n",
    "    return unique_communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stanleysalvatierra/anaconda3/envs/vision_p310/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#First, we load the respective CLIP model\n",
    "model = SentenceTransformer('clip-ViT-B-32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load multiple images to find clusters\n",
    "img_names = image_paths\n",
    "print(\"Images:\", len(img_names))\n",
    "img_emb = model.encode([Image.open(filepath) for filepath in img_names], batch_size=128, convert_to_tensor=True, show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we run the clustering algorithm\n",
    "# With the threshold parameter, we define at which threshold we identify\n",
    "# two images as similar. Set the threshold lower, and you will get larger clusters which have \n",
    "# less similar images in it (e.g. black cat images vs. cat images vs. animal images).\n",
    "# With min_community_size, we define that we only want to have clusters of a certain minimal size\n",
    "# clusters = community_detection(img_emb, threshold=0.99, min_community_size=2)\n",
    "# print(\"Total number of clusters:\", len(clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = 'yueng.jpg'\n",
    "\n",
    "Image.open(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_emb = model.encode(Image.open(test_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute cosine similarities \n",
    "\n",
    "cosine_scores = util.cos_sim(test_emb, img_emb)\n",
    "print(cosine_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision_p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
